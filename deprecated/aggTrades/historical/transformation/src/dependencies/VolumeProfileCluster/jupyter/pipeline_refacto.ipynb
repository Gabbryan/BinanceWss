{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import col, when, to_json, struct, lit, udf, collect_list, round as round_, max as max_, min as min_, sum as sum_, first, last\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, IntegerType, MapType\n",
    "from pyspark.sql import functions as F\n",
    "import boto3\n",
    "import sys\n",
    "from pandas import Timestamp\n",
    "from pandas import DataFrame\n",
    "import time\n",
    "import io\n",
    "import calendar\n",
    "from operator import itemgetter\n",
    "import freqtrade.vendor.qtpylib.indicators as qtpylib\n",
    "import numpy as np  # noqa\n",
    "import pandas as pd  # noqa\n",
    "import pandas_ta as pta\n",
    "import talib.abstract as ta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from freqtrade.strategy import IStrategy\n",
    "from technical.indicators import *\n",
    "from pandas import Timestamp\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "import s3fs\n",
    "import dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Add the directory containing spark_config.py to the Python path\n",
    "module_path = '/root/Cicada-binance/cores/aggTrades/historical/transformation/src/VolumeProfileCluster'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Now you can import the spark_config module\n",
    "import spark_config\n",
    "load_dotenv()\n",
    "\n",
    "ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "SECRET_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "REGION_NAME = os.getenv(\"AWS_REGION_NAME\")\n",
    "BUCKET_NAME = os.getenv(\"AWS_BUCKET_NAME\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/26 23:43:27 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "24/06/26 23:43:28 WARN BasicProfileConfigLoader: Your profile name includes a 'profile ' prefix. This is considered part of the profile name in the Java SDK, so you will need to include this prefix in your profile name when you reference this profile from your Java code.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the SPARK_HOME and PATH environment variables\n",
    "os.environ['SPARK_HOME'] = '/opt/spark'\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['SPARK_HOME'], 'bin')\n",
    "\n",
    "# Add the directory containing spark_config.py to the Python path\n",
    "module_path = '/root/Cicada-binance/cores/aggTrades/historical/transformation/src/VolumeProfileCluster'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import the spark_config module\n",
    "import spark_config\n",
    "\n",
    "# Get the Spark session\n",
    "spark_conf = spark_config.get_spark_session()\n",
    "\n",
    "# Use boto3 and s3fs for additional S3 interaction\n",
    "s3_resource = boto3.resource(\n",
    "    's3',\n",
    "    region_name=os.getenv(\"AWS_REGION_NAME\"),\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=False, key=os.getenv(\"AWS_ACCESS_KEY_ID\"), secret=os.getenv(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "\n",
    "# Read data from S3\n",
    "s3_path = 'xtrus31/cicada-data/DataTransformationAPI/binance-futures/BTCUSDT/aggTrade/15m/aggTrades_processed_2024-01_2024-05.parquet'\n",
    "df = spark_conf.read.parquet(f\"s3a://{s3_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------+-------+-------+------------------+-----------------+-----------------+---------+\n",
      "|       time_rounded|   open|  close|   high|    low|               qty|sum_bid_total_qty|sum_ask_total_qty|footprint|\n",
      "+-------------------+-------+-------+-------+-------+------------------+-----------------+-----------------+---------+\n",
      "|2024-01-01 01:00:00|42313.9|42532.5|42535.0|42289.6|3616.4240000000905|             NULL|             NULL|       {}|\n",
      "|2024-01-01 01:15:00|42532.4|42458.5|42603.2|42449.1|2322.0279999999702|             NULL|             NULL|       {}|\n",
      "|2024-01-01 01:30:00|42458.4|42474.5|42485.7|42386.2|1684.2169999999803|             NULL|             NULL|       {}|\n",
      "|2024-01-01 01:45:00|42474.5|42503.5|42527.2|42449.1| 835.2439999999989|             NULL|             NULL|       {}|\n",
      "|2024-01-01 02:00:00|42503.5|42497.6|42510.4|42462.0| 850.2740000000003|             NULL|             NULL|       {}|\n",
      "|2024-01-01 02:15:00|42497.6|42524.8|42554.9|42497.5|  826.419999999998|             NULL|             NULL|       {}|\n",
      "|2024-01-01 02:30:00|42524.9|42734.3|42832.0|42524.8| 5389.021000000063|             NULL|             NULL|       {}|\n",
      "|2024-01-01 02:45:00|42734.2|42647.9|42750.0|42642.5|1977.6959999999715|             NULL|             NULL|       {}|\n",
      "|2024-01-01 03:00:00|42647.9|42593.7|42676.9|42578.0|1580.5279999999689|             NULL|             NULL|       {}|\n",
      "|2024-01-01 03:15:00|42593.6|42575.6|42610.0|42562.3| 804.4540000000064|             NULL|             NULL|       {}|\n",
      "|2024-01-01 03:30:00|42575.6|42576.2|42647.6|42567.5| 1131.876999999999|             NULL|             NULL|       {}|\n",
      "|2024-01-01 03:45:00|42576.2|42620.4|42630.0|42530.0|1136.2080000000005|             NULL|             NULL|       {}|\n",
      "|2024-01-01 04:00:00|42620.5|42544.2|42630.0|42534.8|1056.8469999999948|             NULL|             NULL|       {}|\n",
      "|2024-01-01 04:15:00|42544.2|42466.9|42544.3|42449.0| 1683.631999999958|             NULL|             NULL|       {}|\n",
      "|2024-01-01 04:30:00|42466.9|42331.9|42493.0|42270.0| 3199.838000000024|             NULL|             NULL|       {}|\n",
      "|2024-01-01 04:45:00|42331.9|42369.8|42391.5|42275.0| 2179.673999999961|             NULL|             NULL|       {}|\n",
      "|2024-01-01 05:00:00|42369.8|42324.6|42414.2|42307.8|1803.6499999999837|             NULL|             NULL|       {}|\n",
      "|2024-01-01 05:15:00|42324.6|42342.9|42353.3|42235.2|2449.9380000000056|             NULL|             NULL|       {}|\n",
      "|2024-01-01 05:30:00|42342.9|42347.3|42381.0|42316.5| 1053.926000000007|             NULL|             NULL|       {}|\n",
      "|2024-01-01 05:45:00|42347.4|42436.6|42439.8|42335.2|1048.9110000000014|             NULL|             NULL|       {}|\n",
      "+-------------------+-------+-------+-------+-------+------------------+-----------------+-----------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config\n",
    "from storage import s3Module\n",
    "from VolumeProfileCluster import DataProcessor, spark_config\n",
    "from VolumeProfileCluster.slack_package import (\n",
    "    SlackChannel,\n",
    "    get_slack_decorators,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = spark_config\n",
    "\n",
    "slack_decorators = get_slack_decorators()\n",
    "data_processor = DataProcessor(ACCESS_KEY, SECRET_KEY, BUCKET_NAME, REGION_NAME)\n",
    "s3_module = s3Module(ACCESS_KEY, SECRET_KEY, BUCKET_NAME, REGION_NAME)\n",
    "\n",
    "# Define the schema for bid and ask data\n",
    "json_schema_bid = StructType(\n",
    "    [\n",
    "        StructField(\"price_level\", DoubleType(), True),\n",
    "        StructField(\"bid_qty\", DoubleType(), True),\n",
    "        StructField(\"bid_trades\", IntegerType(), True),\n",
    "        StructField(\"bid_trades_aggr\", IntegerType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "json_schema_ask = StructType(\n",
    "    [\n",
    "        StructField(\"price_level\", DoubleType(), True),\n",
    "        StructField(\"ask_qty\", DoubleType(), True),\n",
    "        StructField(\"ask_trades\", IntegerType(), True),\n",
    "        StructField(\"ask_trades_aggr\", IntegerType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_type = \"candlestick\"\n",
    "aggregate_trades = True\n",
    "\n",
    "start_year = \"2024\"\n",
    "start_month = \"04\"\n",
    "end_year = \"2024\"\n",
    "end_month = \"05\"\n",
    "timeframe = \"15m\"\n",
    "df = data_processor.calc_df(df, True, timeframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------+-------+-------+------------------+--------------------+--------------------+\n",
      "|       time_rounded|   open|  close|   high|    low|               qty|            foot_bid|            foot_ask|\n",
      "+-------------------+-------+-------+-------+-------+------------------+--------------------+--------------------+\n",
      "|2020-01-01 01:00:00|7189.43|7176.26|7190.52|7172.94|1037.3369999999989|[{\"price_level\":7...|[{\"price_level\":7...|\n",
      "|2020-01-01 01:15:00|7176.22|7172.36|7179.41|7170.69| 707.8329999999991|[{\"price_level\":7...|[{\"price_level\":7...|\n",
      "|2020-01-01 01:30:00|7172.79|7174.83|7179.45|7170.61| 325.2459999999999|[{\"price_level\":7...|[{\"price_level\":7...|\n",
      "|2020-01-01 01:45:00|7174.51|7171.55|7179.36|7170.15| 378.6329999999997|[{\"price_level\":7...|[{\"price_level\":7...|\n",
      "|2020-01-01 02:00:00|7171.43| 7186.6|7188.77| 7171.1| 555.3889999999992|[{\"price_level\":7...|[{\"price_level\":7...|\n",
      "|2020-01-01 02:15:00| 7186.6| 7205.9| 7210.0|7184.16|1332.0769999999925|[{\"price_level\":7...|[{\"price_level\":7...|\n",
      "|2020-01-01 02:30:00|7205.89|7206.63| 7225.0|7201.04|1494.2009999999916|[{\"price_level\":7...|[{\"price_level\":7...|\n",
      "|2020-01-01 02:45:00|7206.62|7210.24|7211.42| 7200.0|483.37100000000027|[{\"price_level\":7...|[{\"price_level\":7...|\n",
      "|2020-01-01 03:00:00|7210.38| 7207.3|7217.76|7206.46| 530.6969999999994|[{\"price_level\":7...|[{\"price_level\":7...|\n",
      "|2020-01-01 03:15:00|7207.33| 7221.5|7232.99|7207.33|1007.3589999999958|[{\"price_level\":7...|[{\"price_level\":7...|\n",
      "|2020-01-01 03:30:00|7221.59|7231.11|7231.11|7221.59| 768.5420000000001|[{\"price_level\":7...|[{\"price_level\":7...|\n",
      "|2020-01-01 03:45:00| 7231.1|7237.99| 7239.3| 7228.0| 921.7669999999976|[{\"price_level\":7...|[{\"price_level\":7...|\n",
      "|2020-01-01 04:00:00|7237.41|7222.55|7239.74|7218.29| 815.6149999999984|[{\"price_level\":7...|[{\"price_level\":7...|\n",
      "|2020-01-01 04:15:00|7222.56|7233.19|7233.61|7222.56|424.10499999999905|[{\"price_level\":7...|[{\"price_level\":7...|\n",
      "|2020-01-01 04:30:00|7233.08|7215.37|7238.46|7215.05| 722.1950000000004|[{\"price_level\":7...|[{\"price_level\":7...|\n",
      "|2020-01-01 04:45:00|7215.37|7221.65|7226.24| 7215.0| 551.3919999999994|[{\"price_level\":7...|[{\"price_level\":7...|\n",
      "|2020-01-01 05:00:00| 7221.8| 7215.3|7225.41| 7213.0|302.66799999999995|[{\"price_level\":7...|[{\"price_level\":7...|\n",
      "|2020-01-01 05:15:00| 7215.3|7216.74| 7218.1|7211.22| 401.1239999999998|[{\"price_level\":7...|[{\"price_level\":7...|\n",
      "|2020-01-01 05:30:00|7216.74| 7217.0|7225.07|7216.74|239.41999999999985|[{\"price_level\":7...|[{\"price_level\":7...|\n",
      "|2020-01-01 05:45:00| 7217.0|7213.86|7223.77|7213.73|233.45400000000015|[{\"price_level\":7...|[{\"price_level\":7...|\n",
      "+-------------------+-------+-------+-------+-------+------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------------+----------------+---------------------+\n",
      "|       time_rounded|price_level|     total_bid_qty|total_bid_trades|total_bid_trades_aggr|\n",
      "+-------------------+-----------+------------------+----------------+---------------------+\n",
      "|2020-01-01 01:00:00|     7173.0|17.825999999999997|              16|                   16|\n",
      "|2020-01-01 01:00:00|     7174.0|15.869000000000002|              30|                   30|\n",
      "|2020-01-01 01:00:00|     7175.0| 29.78100000000001|              39|                   39|\n",
      "|2020-01-01 01:00:00|     7176.0| 51.82800000000002|              64|                   64|\n",
      "|2020-01-01 01:00:00|     7177.0| 52.13700000000002|              60|                   60|\n",
      "|2020-01-01 01:00:00|     7178.0|105.05199999999999|             100|                  100|\n",
      "|2020-01-01 01:00:00|     7179.0| 71.12200000000001|              67|                   67|\n",
      "|2020-01-01 01:00:00|     7180.0| 46.92099999999999|              54|                   54|\n",
      "|2020-01-01 01:00:00|     7181.0| 38.78600000000001|              38|                   38|\n",
      "|2020-01-01 01:00:00|     7182.0|            57.685|              54|                   54|\n",
      "|2020-01-01 01:00:00|     7183.0|35.980000000000004|              29|                   29|\n",
      "|2020-01-01 01:00:00|     7184.0| 27.68199999999999|              27|                   27|\n",
      "|2020-01-01 01:00:00|     7185.0|            36.804|              35|                   35|\n",
      "|2020-01-01 01:00:00|     7186.0|11.467999999999998|              18|                   18|\n",
      "|2020-01-01 01:00:00|     7187.0|            12.878|              17|                   17|\n",
      "|2020-01-01 01:00:00|     7188.0|             6.385|               9|                    9|\n",
      "|2020-01-01 01:00:00|     7189.0|16.823999999999998|               8|                    8|\n",
      "|2020-01-01 01:00:00|     7190.0|            18.558|               6|                    6|\n",
      "|2020-01-01 01:15:00|     7171.0|52.401999999999994|              29|                   29|\n",
      "|2020-01-01 01:15:00|     7172.0|55.815999999999995|              55|                   55|\n",
      "+-------------------+-----------+------------------+----------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|       time_rounded| sum_bid_total_qty|\n",
      "+-------------------+------------------+\n",
      "|2020-01-01 01:00:00|           653.586|\n",
      "|2020-01-01 01:15:00|           433.223|\n",
      "|2020-01-01 01:30:00|162.71000000000004|\n",
      "|2020-01-01 01:45:00|203.33200000000002|\n",
      "|2020-01-01 02:00:00|           218.562|\n",
      "|2020-01-01 02:15:00|           474.085|\n",
      "|2020-01-01 02:30:00| 648.2780000000001|\n",
      "|2020-01-01 02:45:00|           183.235|\n",
      "|2020-01-01 03:00:00|           303.185|\n",
      "|2020-01-01 03:15:00|           372.194|\n",
      "|2020-01-01 03:30:00|           356.481|\n",
      "|2020-01-01 03:45:00|422.35999999999996|\n",
      "|2020-01-01 04:00:00|473.55799999999994|\n",
      "|2020-01-01 04:15:00|187.30799999999996|\n",
      "|2020-01-01 04:30:00|420.58700000000005|\n",
      "|2020-01-01 04:45:00|           186.789|\n",
      "|2020-01-01 05:00:00|           148.988|\n",
      "|2020-01-01 05:15:00|191.71900000000002|\n",
      "|2020-01-01 05:30:00|           132.815|\n",
      "|2020-01-01 05:45:00|116.27999999999999|\n",
      "+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_bid_agg_sorted, df_bid_total_qty = data_processor.process_foot_data(\n",
    "    df, \"foot_bid\", json_schema_bid, \"bid\"\n",
    ")\n",
    "df_bid_agg_sorted.show()\n",
    "df_bid_total_qty.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN.PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "import boto3\n",
    "import pytz\n",
    "import schedule\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from config import config\n",
    "from storage import s3Module\n",
    "from VolumeProfileCluster import DataProcessor, spark_config\n",
    "from VolumeProfileCluster.slack_package import (\n",
    "    SlackChannel,\n",
    "    get_slack_decorators,\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "ACCESS_KEY = config.ACCESS_KEY\n",
    "SECRET_KEY = config.SECRET_KEY\n",
    "REGION_NAME = config.REGION_NAME\n",
    "BUCKET_NAME = config.BUCKET_NAME\n",
    "BEARER_TOKEN = config.BEARER_TOKEN\n",
    "SLACK_WEBHOOK_URL = config.SLACK_WEBHOOK_URL\n",
    "PROCESSED_FILES_LOG = config.PROCESSED_FILES_LOG\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.FileHandler(\"application.log\"), logging.StreamHandler()],\n",
    ")\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = spark_config\n",
    "\n",
    "slack_decorators = get_slack_decorators()\n",
    "slack_channel = SlackChannel(SLACK_WEBHOOK_URL)\n",
    "data_processor = DataProcessor(ACCESS_KEY, SECRET_KEY, BUCKET_NAME, REGION_NAME)\n",
    "s3_module = s3Module(ACCESS_KEY, SECRET_KEY, BUCKET_NAME, REGION_NAME)\n",
    "\n",
    "\n",
    "def transform_footprint(row):\n",
    "    logging.info(\"Transforming footprint\")\n",
    "    try:\n",
    "        bid_data = row[\"aggregated_foot_bid\"]\n",
    "        ask_data = row[\"aggregated_foot_ask\"]\n",
    "        aggregated_data = {}\n",
    "\n",
    "        if bid_data:\n",
    "            for bid_item in bid_data:\n",
    "                for price, data in bid_item.items():\n",
    "                    if price not in aggregated_data:\n",
    "                        aggregated_data[price] = {\n",
    "                            \"bid_qty\": 0,\n",
    "                            \"bid_trades\": 0,\n",
    "                            \"ask_qty\": 0,\n",
    "                            \"ask_trades\": 0,\n",
    "                        }\n",
    "                    aggregated_data[price][\"bid_qty\"] += data[\"bid_qty\"]\n",
    "                    aggregated_data[price][\"bid_trades\"] += data[\"bid_trades\"]\n",
    "\n",
    "        if ask_data:\n",
    "            for ask_item in ask_data:\n",
    "                for price, data in ask_item.items():\n",
    "                    if price not in aggregated_data:\n",
    "                        aggregated_data[price] = {\n",
    "                            \"bid_qty\": 0,\n",
    "                            \"bid_trades\": 0,\n",
    "                            \"ask_qty\": 0,\n",
    "                            \"ask_trades\": 0,\n",
    "                        }\n",
    "                    aggregated_data[price][\"ask_qty\"] += data[\"ask_qty\"]\n",
    "                    aggregated_data[price][\"ask_trades\"] += data[\"ask_trades\"]\n",
    "\n",
    "        logging.info(\"Footprint transformation successful\")\n",
    "        return aggregated_data\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in transform_footprint: {e}\")\n",
    "        logging.error(\"Traceback details:\", exc_info=True)\n",
    "        return {}\n",
    "\n",
    "\n",
    "@slack_decorators.notify_with_link(\n",
    "    header=\"A new Parquet file is available 🗂️\",\n",
    "    message=\"The pipeline has updated the daily agg Trade file\",\n",
    "    color=\"#6a0dad\",\n",
    ")\n",
    "def data_pipeline_exec(\n",
    "    data_processor,\n",
    "    df_trades,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    symbol,\n",
    "    exchange,\n",
    "    chart_type,\n",
    "    timeframe,\n",
    "    aggregate_trades=1,\n",
    "):\n",
    "    try:\n",
    "        logging.info(\"Starting data pipeline execution\")\n",
    "\n",
    "        logging.info(\"Process footprint\")\n",
    "        processed_df = data_processor.calc_footprint(\n",
    "            df_trades, True, chart_type, symbol, timeframe\n",
    "        )\n",
    "\n",
    "        logging.info(\"Process OHLCV\")\n",
    "        df_ohlcv = data_processor.calc_ohlcv(df_trades, timeframe)\n",
    "\n",
    "        logging.info(\"Calculate df_bid_agg, df_ask_agg\")\n",
    "        df_bid_agg, df_bid_qty, df_ask_agg, df_ask_qty = (\n",
    "            data_processor.aggregate_bid_ask_data(processed_df)\n",
    "        )\n",
    "\n",
    "        logging.info(\"Bid footprint transformation\")\n",
    "        df_footprint_bids = data_processor.transform_and_aggregate_footprint(\n",
    "            df_bid_agg, \"bid\"\n",
    "        )\n",
    "\n",
    "        logging.info(\"Ask footprint transformation\")\n",
    "        df_footprint_asks = data_processor.transform_and_aggregate_footprint(\n",
    "            df_ask_agg, \"ask\"\n",
    "        )\n",
    "\n",
    "        logging.info(\"Joining final dataframe\")\n",
    "        df_joined_final = data_processor.join_bid_ask_footprints(\n",
    "            df_ohlcv, df_bid_qty, df_ask_qty, df_footprint_bids, df_footprint_asks\n",
    "        )\n",
    "\n",
    "        logging.info(\"Transforming footprint data in final dataframe\")\n",
    "        df_joined_final = df_joined_final.withColumn(\n",
    "            \"footprint\",\n",
    "            F.udf(lambda row: transform_footprint(row))(\n",
    "                F.struct([df_joined_final[x] for x in df_joined_final.columns])\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        logging.info(\"Saving data to S3 and local filesystem\")\n",
    "        data_processor.save_dataframe_to_s3_fs(\n",
    "            df_joined_final, symbol, exchange, timeframe, start_date, end_date\n",
    "        )\n",
    "\n",
    "        logging.info(\"Data pipeline execution completed successfully\")\n",
    "        return df_joined_final\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An exception occurred during data pipeline execution: {str(e)}\")\n",
    "        logging.error(\"Traceback details:\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "\n",
    "def run_data_pipeline(\n",
    "    data_processor,\n",
    "    df_trades,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    symbol,\n",
    "    exchange,\n",
    "    chart_type,\n",
    "    timeframe,\n",
    "):\n",
    "    try:\n",
    "        df_final = data_pipeline_exec(\n",
    "            data_processor,\n",
    "            df_trades,\n",
    "            start_date,\n",
    "            end_date,\n",
    "            symbol,\n",
    "            exchange,\n",
    "            chart_type,\n",
    "            timeframe,\n",
    "        )\n",
    "        logging.info(\"Data pipeline execution completed.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An exception occurred during data pipeline execution: {str(e)}\")\n",
    "        logging.error(\"Traceback details:\", exc_info=True)\n",
    "\n",
    "\n",
    "def process_year_data(\n",
    "    data_processor,\n",
    "    spark,\n",
    "    s3_paths_by_year,\n",
    "    year,\n",
    "    symbol,\n",
    "    exchange,\n",
    "    chart_type,\n",
    "    timeframes,\n",
    "    start_month,\n",
    "    end_month,\n",
    "):\n",
    "    try:\n",
    "        logging.info(f\"Processing data for year: {year}\")\n",
    "\n",
    "        df_year = None\n",
    "        for month, s3_path in s3_paths_by_year[year]:\n",
    "            try:\n",
    "                logging.info(f\"Reading data from {s3_path}\")\n",
    "\n",
    "                df_month = spark.read.parquet(s3_path)\n",
    "\n",
    "                new_column_names = [\n",
    "                    \"agg_trade_id\",\n",
    "                    \"price\",\n",
    "                    \"quantity\",\n",
    "                    \"first_trade_id\",\n",
    "                    \"last_trade_id\",\n",
    "                    \"time\",\n",
    "                    \"is_buyer_maker\",\n",
    "                ]\n",
    "                for old_name, new_name in zip(df_month.columns, new_column_names):\n",
    "                    df_month = df_month.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "                header_row = df_month.filter(\n",
    "                    col(new_column_names[0]) == \"agg_trade_id\"\n",
    "                ).first()\n",
    "                if header_row:\n",
    "                    df_month = df_month.filter(\n",
    "                        col(new_column_names[0]) != \"agg_trade_id\"\n",
    "                    )\n",
    "\n",
    "                if df_year is None:\n",
    "                    df_year = df_month\n",
    "                else:\n",
    "                    df_year = df_year.union(df_month)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to read data from {s3_path}: {e}\")\n",
    "\n",
    "        if df_year is None:\n",
    "            logging.info(f\"No data available for year {year}. Skipping.\")\n",
    "            return\n",
    "\n",
    "        for timeframe in timeframes:\n",
    "            logging.info(f\"Processing timeframe: {timeframe}\")\n",
    "            try:\n",
    "                run_data_pipeline(\n",
    "                    data_processor,\n",
    "                    df_year,\n",
    "                    f\"{year}-{start_month:02d}\",\n",
    "                    f\"{year}-{end_month:02d}\",\n",
    "                    symbol,\n",
    "                    exchange,\n",
    "                    chart_type,\n",
    "                    timeframe,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logging.error(\n",
    "                    f\"An exception occurred while processing data for {year}, {timeframe}: {e}\"\n",
    "                )\n",
    "                logging.error(\"Traceback details:\", exc_info=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\n",
    "            f\"An exception occurred while processing data for year {year}: {e}\"\n",
    "        )\n",
    "        logging.error(\"Traceback details:\", exc_info=True)\n",
    "\n",
    "\n",
    "def daily_update():\n",
    "    start_time = time.time()\n",
    "\n",
    "    chart_type = \"candlestick\"\n",
    "    aggregate_trades = True\n",
    "\n",
    "    start_year = \"2024\"\n",
    "    start_month = \"04\"\n",
    "    end_year = \"2024\"\n",
    "    end_month = \"05\"\n",
    "\n",
    "    paris_tz = pytz.timezone(\"Europe/Paris\")\n",
    "    current_time = datetime.datetime.now(tz=paris_tz)\n",
    "    logging.info(f\"Current Paris time: {current_time}\")\n",
    "\n",
    "    cryptos = s3_module.get_cryptos(\n",
    "        BUCKET_NAME, \"cicada-data/HistoricalTradeAggregator/binance_futures/\"\n",
    "    )\n",
    "    timeframes = [\"15mn\"]\n",
    "\n",
    "    all_s3_paths = []\n",
    "    total_files = 0\n",
    "    total_size = 0\n",
    "\n",
    "    slack_channel.send_message(\n",
    "        \"🚀 Daily Update Initiated\",\n",
    "        f\"_\\\"You think I'm a parasite, don't you? But I just want to make money.\\\"_ - Jared Vennett\\n\\n\"\n",
    "        f\"*Starting the daily transformation with* `{len(cryptos)}` *cryptocurrencies across* `{len(timeframes)}` *timeframes.*\\n\"\n",
    "        f\"🕰️ *Start Time*: {current_time}\",\n",
    "    )\n",
    "\n",
    "    for crypto in cryptos:\n",
    "        crypto_start_time = time.time()\n",
    "        crypto_files = 0\n",
    "        crypto_size = 0\n",
    "\n",
    "        s3_paths = s3_module.generate_s3_paths(\n",
    "            crypto.replace(\"/\", \"\"), start_year, end_year, start_month, end_month\n",
    "        )\n",
    "        all_s3_paths.extend(s3_paths)\n",
    "\n",
    "        s3_paths_by_year = {}\n",
    "        for year, month, s3_path in all_s3_paths:\n",
    "            if year not in s3_paths_by_year:\n",
    "                s3_paths_by_year[year] = []\n",
    "            s3_paths_by_year[year].append((month, s3_path))\n",
    "\n",
    "        for year in s3_paths_by_year:\n",
    "            year_start_month = int(start_month) if year == int(start_year) else 1\n",
    "            year_end_month = int(end_month) if year == int(end_year) else 12\n",
    "            process_year_data(\n",
    "                data_processor,\n",
    "                spark,\n",
    "                s3_paths_by_year,\n",
    "                year,\n",
    "                crypto,\n",
    "                \"binance-futures\",\n",
    "                chart_type,\n",
    "                timeframes,\n",
    "                year_start_month,\n",
    "                year_end_month,\n",
    "            )\n",
    "\n",
    "        crypto_files += len(s3_paths)\n",
    "\n",
    "        total_files += crypto_files\n",
    "        total_size += 0\n",
    "\n",
    "        crypto_duration = time.time() - crypto_start_time\n",
    "        slack_channel.send_message(\n",
    "            f\"✅ Crypto Transformation Complete: {crypto}\",\n",
    "            f'_\"This business kills the part of life that is essential, the part that has nothing to do with business.\"_ - Ben Rickert\\n\\n'\n",
    "            f\"Successfully processed *{crypto_files}* files for *{crypto}* in *{crypto_duration:.2f} seconds*. The data now occupies a total of *{crypto_size / (1024 ** 3):.2f} GB*.\\n\"\n",
    "            f\"📈 *Files Processed*: {crypto_files}\\n\"\n",
    "            f\"🕰️ *Processing Time*: {crypto_duration:.2f} seconds\\n\"\n",
    "            f\"💾 *Total Data Size*: {crypto_size / (1024 ** 3):.2f} GB\",\n",
    "        )\n",
    "\n",
    "    total_duration = time.time() - start_time\n",
    "    slack_channel.send_message(\n",
    "        \"🎬 Daily Transformation Complete\",\n",
    "        f'_\"In the end they knew.\"_ - Narrator\\n\\n'\n",
    "        f\"All cryptocurrencies have been processed. Here are the final stats for today's update:\\n\"\n",
    "        f\"📈 *Total Files Processed*: {total_files}\\n\"\n",
    "        f\"🕰️ *Total Processing Time*: {total_duration / 60:.2f} minutes\\n\"\n",
    "        f\"💾 *Total Data Size*: {total_size / (1024 ** 3):.2f} GB\\n\"\n",
    "        f\"Outstanding work, everyone! 🌟\",\n",
    "    )\n",
    "\n",
    "\n",
    "schedule.every().day.at(\"10:00\").do(daily_update)\n",
    "\n",
    "schedule.run_all()\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
