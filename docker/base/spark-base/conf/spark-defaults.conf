# Application configuration
spark.app.name DataAggregatorSpark
spark.master spark://spark-master:7077

# Dynamic allocation configuration
spark.dynamicAllocation.enabled true
spark.dynamicAllocation.minExecutors 2
spark.dynamicAllocation.maxExecutors 50
spark.dynamicAllocation.initialExecutors 5
spark.dynamicAllocation.executorIdleTimeout 60s
spark.dynamicAllocation.schedulerBacklogTimeout 1s
spark.dynamicAllocation.sustainedSchedulerBacklogTimeout 1s
spark.dynamicAllocation.executorAllocationRatio 1

# Resource allocation configuration
spark.executor.memory 10g
spark.executor.cores 4
spark.cores.max 64
spark.driver.memory 8g

# Parallelism and shuffle settings
spark.sql.shuffle.partitions 200
spark.default.parallelism 200

# Compression settings
spark.shuffle.compress true
spark.shuffle.spill.compress true
spark.io.compression.codec lz4

# Memory management
spark.memory.fraction 0.75
spark.memory.storageFraction 0.25

# Speculative execution
spark.speculation true
spark.speculation.interval 100ms
spark.speculation.multiplier 1.5
spark.speculation.quantile 0.90

# Other settings
spark.executor.extraJavaOptions -XX:+UseG1GC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps

# Set the log level to DEBUG to troubleshoot issues
spark.driver.extraJavaOptions -Dlog4j.configuration=file:/opt/spark/conf/log4j.properties
spark.executor.extraJavaOptions -Dlog4j.configuration=file:/opt/spark/conf/log4j.properties

# AWS credentials configuration (ensure these are set securely, e.g., using environment variables)
spark.hadoop.fs.s3a.access.key AKIAS625BHFC5UUKQVYE
spark.hadoop.fs.s3a.secret.key 0NbRBW346WqQcqgk/20Z0IgGE57eEASJNZ6I7xe9

# Configure S3 endpoint and region
spark.hadoop.fs.s3a.endpoint s3.amazonaws.com
spark.hadoop.fs.s3a.endpoint.region eu-north-1

# Retry policy settings
spark.hadoop.fs.s3a.retry.limit 7
spark.hadoop.fs.s3a.retry.interval 500ms

# Configure connection pool settings to prevent timeout issues
spark.hadoop.fs.s3a.connection.maximum 64
spark.hadoop.fs.s3a.threads.max 64
spark.hadoop.fs.s3a.connection.ttl 300000

# Additional configuration to handle read-during-overwrite scenarios
spark.hadoop.fs.s3a.change.detection.mode none

# Set logging level for debugging
log4j.logger.org.apache.hadoop.fs.s3a=DEBUG
log4j.logger.software.amazon.awssdk.request=DEBUG
