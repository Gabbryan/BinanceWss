{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c774f-09fa-4693-8b3a-18c801691f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/28 22:12:28 INFO SparkContext: Running Spark version 3.5.1\n",
      "24/06/28 22:12:28 INFO SparkContext: OS info Linux, 6.2.0-39-generic, amd64\n",
      "24/06/28 22:12:28 INFO SparkContext: Java version 17.0.9\n",
      "24/06/28 22:12:28 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "24/06/28 22:12:28 INFO ResourceUtils: ==============================================================\n",
      "24/06/28 22:12:28 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/06/28 22:12:28 INFO ResourceUtils: ==============================================================\n",
      "24/06/28 22:12:28 INFO SparkContext: Submitted application: Advanced PySpark Test\n",
      "24/06/28 22:12:28 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 4096, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/06/28 22:12:28 INFO ResourceProfile: Limiting resource is cpus at 4 tasks per executor\n",
      "24/06/28 22:12:28 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/06/28 22:12:28 INFO SecurityManager: Changing view acls to: root\n",
      "24/06/28 22:12:28 INFO SecurityManager: Changing modify acls to: root\n",
      "24/06/28 22:12:28 INFO SecurityManager: Changing view acls groups to: \n",
      "24/06/28 22:12:28 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/06/28 22:12:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "24/06/28 22:12:28 INFO Utils: Successfully started service 'sparkDriver' on port 7078.\n",
      "24/06/28 22:12:28 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/06/28 22:12:28 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/06/28 22:12:28 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/06/28 22:12:28 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/06/28 22:12:28 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/06/28 22:12:28 INFO DiskBlockManager: Created local directory at /tmp/spark-temp/blockmgr-8c6c87f7-b3b7-413b-b7ac-3d6590be04b5\n",
      "24/06/28 22:12:28 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "24/06/28 22:12:28 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/06/28 22:12:28 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/06/28 22:12:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/06/28 22:12:28 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n",
      "24/06/28 22:12:28 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 2 ms (0 ms spent in bootstraps)\n",
      "24/06/28 22:12:28 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240628221228-0002\n",
      "24/06/28 22:12:28 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240628221228-0002/0 on worker-20240628220513-127.0.0.1-39355 (127.0.0.1:39355) with 4 core(s)\n",
      "24/06/28 22:12:28 INFO StandaloneSchedulerBackend: Granted executor ID app-20240628221228-0002/0 on hostPort 127.0.0.1:39355 with 4 core(s), 4.0 GiB RAM\n",
      "24/06/28 22:12:28 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240628221228-0002/1 on worker-20240628220513-127.0.0.1-39355 (127.0.0.1:39355) with 4 core(s)\n",
      "24/06/28 22:12:28 INFO StandaloneSchedulerBackend: Granted executor ID app-20240628221228-0002/1 on hostPort 127.0.0.1:39355 with 4 core(s), 4.0 GiB RAM\n",
      "24/06/28 22:12:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44785.\n",
      "24/06/28 22:12:28 INFO NettyBlockTransferService: Server created on localhost 0.0.0.0:44785\n",
      "24/06/28 22:12:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/06/28 22:12:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 44785, None)\n",
      "24/06/28 22:12:28 INFO BlockManagerMasterEndpoint: Registering block manager localhost:44785 with 434.4 MiB RAM, BlockManagerId(driver, localhost, 44785, None)\n",
      "24/06/28 22:12:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 44785, None)\n",
      "24/06/28 22:12:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 44785, None)\n",
      "24/06/28 22:12:28 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "24/06/28 22:12:28 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240628221228-0002/0 is now RUNNING\n",
      "24/06/28 22:12:28 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240628221228-0002/1 is now RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/28 22:12:29 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/06/28 22:12:29 INFO SharedState: Warehouse path is 'file:/root/Trustia/Cicada-binance/spark-warehouse'.\n",
      "24/06/28 22:12:30 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (127.0.0.1:41994) with ID 0,  ResourceProfileId 0\n",
      "24/06/28 22:12:30 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (127.0.0.1:42000) with ID 1,  ResourceProfileId 0\n",
      "24/06/28 22:12:30 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:36899 with 2.2 GiB RAM, BlockManagerId(0, 127.0.0.1, 36899, None)\n",
      "24/06/28 22:12:30 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:42841 with 2.2 GiB RAM, BlockManagerId(1, 127.0.0.1, 42841, None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/28 22:12:44 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/06/28 22:12:44 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/28 22:12:44 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/06/28 22:12:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/28 22:12:44 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/28 22:12:44 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/28 22:12:44 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 13.2 KiB, free 434.4 MiB)\n",
      "24/06/28 22:12:44 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 434.4 MiB)\n",
      "24/06/28 22:12:44 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:44785 (size: 6.8 KiB, free: 434.4 MiB)\n",
      "24/06/28 22:12:44 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "24/06/28 22:12:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/28 22:12:44 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/06/28 22:12:44 WARN TaskSetManager: Stage 0 contains a task of very large size (1841 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/28 22:12:44 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (127.0.0.1, executor 1, partition 0, PROCESS_LOCAL, 1885667 bytes) \n",
      "24/06/28 22:12:44 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:42841 (size: 6.8 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1214 ms on 127.0.0.1 (executor 1) (1/1)\n",
      "24/06/28 22:12:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/06/28 22:12:45 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 50503\n",
      "24/06/28 22:12:45 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 1.219 s\n",
      "24/06/28 22:12:45 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/28 22:12:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/06/28 22:12:45 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 1.220942 s\n",
      "24/06/28 22:12:45 INFO DAGScheduler: Registering RDD 8 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "24/06/28 22:12:45 INFO DAGScheduler: Got map stage job 1 (showString at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "24/06/28 22:12:45 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/06/28 22:12:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/28 22:12:45 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/28 22:12:45 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/28 22:12:45 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 22.8 KiB, free 434.4 MiB)\n",
      "24/06/28 22:12:45 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.3 MiB)\n",
      "24/06/28 22:12:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:44785 (size: 10.3 KiB, free: 434.4 MiB)\n",
      "24/06/28 22:12:45 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "24/06/28 22:12:45 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[8] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "24/06/28 22:12:45 INFO TaskSchedulerImpl: Adding task set 1.0 with 8 tasks resource profile 0\n",
      "24/06/28 22:12:45 WARN TaskSetManager: Stage 1 contains a task of very large size (1841 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/28 22:12:45 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (127.0.0.1, executor 0, partition 0, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:45 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (127.0.0.1, executor 1, partition 1, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:44785 in memory (size: 6.8 KiB, free: 434.4 MiB)\n",
      "24/06/28 22:12:45 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (127.0.0.1, executor 0, partition 2, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:45 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (127.0.0.1, executor 1, partition 3, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:45 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (127.0.0.1, executor 0, partition 4, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:45 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (127.0.0.1, executor 1, partition 5, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:45 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (127.0.0.1, executor 0, partition 6, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:45 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (127.0.0.1, executor 1, partition 7, PROCESS_LOCAL, 1894328 bytes) \n",
      "24/06/28 22:12:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:42841 in memory (size: 6.8 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:42841 (size: 10.3 KiB, free: 2.2 GiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------+\n",
      "|IntegerColumn|        FloatColumn|Category|\n",
      "+-------------+-------------------+--------+\n",
      "|           96|0.17959478724299838|       A|\n",
      "|           72|0.26446945233007657|       B|\n",
      "|           84|0.17098642346686066|       C|\n",
      "|           40| 0.6116877547081923|       B|\n",
      "|           32| 0.7479216165525343|       B|\n",
      "+-------------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Statistics of FloatColumn:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/28 22:12:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:36899 (size: 10.3 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:46 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 486 ms on 127.0.0.1 (executor 1) (1/8)\n",
      "24/06/28 22:12:46 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 482 ms on 127.0.0.1 (executor 1) (2/8)\n",
      "24/06/28 22:12:46 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 485 ms on 127.0.0.1 (executor 1) (3/8)\n",
      "24/06/28 22:12:46 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 481 ms on 127.0.0.1 (executor 1) (4/8)\n",
      "24/06/28 22:12:47 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 1646 ms on 127.0.0.1 (executor 0) (5/8)\n",
      "24/06/28 22:12:47 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 1642 ms on 127.0.0.1 (executor 0) (6/8)\n",
      "24/06/28 22:12:47 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1653 ms on 127.0.0.1 (executor 0) (7/8)\n",
      "24/06/28 22:12:47 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 1644 ms on 127.0.0.1 (executor 0) (8/8)\n",
      "24/06/28 22:12:47 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/06/28 22:12:47 INFO DAGScheduler: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 1.659 s\n",
      "24/06/28 22:12:47 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/06/28 22:12:47 INFO DAGScheduler: running: Set()\n",
      "24/06/28 22:12:47 INFO DAGScheduler: waiting: Set()\n",
      "24/06/28 22:12:47 INFO DAGScheduler: failed: Set()\n",
      "24/06/28 22:12:47 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/06/28 22:12:47 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/28 22:12:47 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/06/28 22:12:47 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
      "24/06/28 22:12:47 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/28 22:12:47 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/28 22:12:47 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 19.3 KiB, free 434.3 MiB)\n",
      "24/06/28 22:12:47 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 434.3 MiB)\n",
      "24/06/28 22:12:47 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:44785 (size: 8.0 KiB, free: 434.4 MiB)\n",
      "24/06/28 22:12:47 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "24/06/28 22:12:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/28 22:12:47 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "24/06/28 22:12:47 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 9) (127.0.0.1, executor 0, partition 0, NODE_LOCAL, 7619 bytes) \n",
      "24/06/28 22:12:47 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:36899 (size: 8.0 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:47 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 127.0.0.1:41994\n",
      "24/06/28 22:12:47 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 9) in 206 ms on 127.0.0.1 (executor 0) (1/1)\n",
      "24/06/28 22:12:47 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/06/28 22:12:47 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.211 s\n",
      "24/06/28 22:12:47 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/28 22:12:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "24/06/28 22:12:47 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.213545 s\n",
      "24/06/28 22:12:47 INFO DAGScheduler: Registering RDD 13 (collect at /tmp/ipykernel_14601/2935220721.py:54) as input to shuffle 1\n",
      "24/06/28 22:12:47 INFO DAGScheduler: Got map stage job 3 (collect at /tmp/ipykernel_14601/2935220721.py:54) with 8 output partitions\n",
      "24/06/28 22:12:47 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (collect at /tmp/ipykernel_14601/2935220721.py:54)\n",
      "24/06/28 22:12:47 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/28 22:12:47 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/28 22:12:47 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[13] at collect at /tmp/ipykernel_14601/2935220721.py:54), which has no missing parents\n",
      "24/06/28 22:12:47 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 22.8 KiB, free 434.3 MiB)\n",
      "24/06/28 22:12:47 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.3 MiB)\n",
      "24/06/28 22:12:47 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:44785 (size: 10.3 KiB, free: 434.4 MiB)\n",
      "24/06/28 22:12:47 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
      "24/06/28 22:12:47 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[13] at collect at /tmp/ipykernel_14601/2935220721.py:54) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "24/06/28 22:12:47 INFO TaskSchedulerImpl: Adding task set 4.0 with 8 tasks resource profile 0\n",
      "24/06/28 22:12:47 WARN TaskSetManager: Stage 4 contains a task of very large size (1841 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/28 22:12:47 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 10) (127.0.0.1, executor 0, partition 0, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:47 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 11) (127.0.0.1, executor 1, partition 1, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:47 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 12) (127.0.0.1, executor 0, partition 2, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:47 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 13) (127.0.0.1, executor 1, partition 3, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:47 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 14) (127.0.0.1, executor 0, partition 4, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:47 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 15) (127.0.0.1, executor 1, partition 5, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:47 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 16) (127.0.0.1, executor 0, partition 6, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:47 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 17) (127.0.0.1, executor 1, partition 7, PROCESS_LOCAL, 1894328 bytes) \n",
      "24/06/28 22:12:47 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:44785 in memory (size: 10.3 KiB, free: 434.4 MiB)\n",
      "24/06/28 22:12:47 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 127.0.0.1:42841 in memory (size: 10.3 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:47 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 127.0.0.1:36899 in memory (size: 10.3 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:47 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:42841 (size: 10.3 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:47 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:36899 (size: 10.3 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:47 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:44785 in memory (size: 8.0 KiB, free: 434.4 MiB)\n",
      "24/06/28 22:12:47 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 127.0.0.1:36899 in memory (size: 8.0 KiB, free: 2.2 GiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|     Average_Float|      StdDev_Float|\n",
      "+------------------+------------------+\n",
      "|0.5004873761859209|0.2886989718775669|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/28 22:12:47 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 11) in 271 ms on 127.0.0.1 (executor 1) (1/8)\n",
      "24/06/28 22:12:48 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 13) in 293 ms on 127.0.0.1 (executor 1) (2/8)\n",
      "24/06/28 22:12:48 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 17) in 297 ms on 127.0.0.1 (executor 1) (3/8)\n",
      "24/06/28 22:12:48 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 15) in 325 ms on 127.0.0.1 (executor 1) (4/8)\n",
      "24/06/28 22:12:48 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 12) in 333 ms on 127.0.0.1 (executor 0) (5/8)\n",
      "24/06/28 22:12:48 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 14) in 333 ms on 127.0.0.1 (executor 0) (6/8)\n",
      "24/06/28 22:12:48 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 16) in 339 ms on 127.0.0.1 (executor 0) (7/8)\n",
      "24/06/28 22:12:48 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 10) in 344 ms on 127.0.0.1 (executor 0) (8/8)\n",
      "24/06/28 22:12:48 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/06/28 22:12:48 INFO DAGScheduler: ShuffleMapStage 4 (collect at /tmp/ipykernel_14601/2935220721.py:54) finished in 0.348 s\n",
      "24/06/28 22:12:48 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/06/28 22:12:48 INFO DAGScheduler: running: Set()\n",
      "24/06/28 22:12:48 INFO DAGScheduler: waiting: Set()\n",
      "24/06/28 22:12:48 INFO DAGScheduler: failed: Set()\n",
      "24/06/28 22:12:48 INFO SparkContext: Starting job: collect at /tmp/ipykernel_14601/2935220721.py:54\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Got job 4 (collect at /tmp/ipykernel_14601/2935220721.py:54) with 1 output partitions\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /tmp/ipykernel_14601/2935220721.py:54)\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[16] at collect at /tmp/ipykernel_14601/2935220721.py:54), which has no missing parents\n",
      "24/06/28 22:12:48 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 19.1 KiB, free 434.3 MiB)\n",
      "24/06/28 22:12:48 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 434.3 MiB)\n",
      "24/06/28 22:12:48 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:44785 (size: 7.9 KiB, free: 434.4 MiB)\n",
      "24/06/28 22:12:48 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[16] at collect at /tmp/ipykernel_14601/2935220721.py:54) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/28 22:12:48 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "24/06/28 22:12:48 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 18) (127.0.0.1, executor 1, partition 0, NODE_LOCAL, 7619 bytes) \n",
      "24/06/28 22:12:48 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:42841 (size: 7.9 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:48 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 127.0.0.1:42000\n",
      "24/06/28 22:12:48 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 18) in 142 ms on 127.0.0.1 (executor 1) (1/1)\n",
      "24/06/28 22:12:48 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "24/06/28 22:12:48 INFO DAGScheduler: ResultStage 6 (collect at /tmp/ipykernel_14601/2935220721.py:54) finished in 0.147 s\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/28 22:12:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Job 4 finished: collect at /tmp/ipykernel_14601/2935220721.py:54, took 0.148704 s\n",
      "24/06/28 22:12:48 INFO SparkContext: Starting job: collect at /tmp/ipykernel_14601/2935220721.py:55\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Got job 5 (collect at /tmp/ipykernel_14601/2935220721.py:55) with 1 output partitions\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Final stage: ResultStage 8 (collect at /tmp/ipykernel_14601/2935220721.py:55)\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[18] at collect at /tmp/ipykernel_14601/2935220721.py:55), which has no missing parents\n",
      "24/06/28 22:12:48 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 19.1 KiB, free 434.3 MiB)\n",
      "24/06/28 22:12:48 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 434.3 MiB)\n",
      "24/06/28 22:12:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:44785 (size: 8.0 KiB, free: 434.4 MiB)\n",
      "24/06/28 22:12:48 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[18] at collect at /tmp/ipykernel_14601/2935220721.py:55) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/28 22:12:48 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "24/06/28 22:12:48 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 19) (127.0.0.1, executor 0, partition 0, NODE_LOCAL, 7619 bytes) \n",
      "24/06/28 22:12:48 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 127.0.0.1:36899 (size: 8.0 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:48 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 127.0.0.1:41994\n",
      "24/06/28 22:12:48 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 19) in 56 ms on 127.0.0.1 (executor 0) (1/1)\n",
      "24/06/28 22:12:48 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "24/06/28 22:12:48 INFO DAGScheduler: ResultStage 8 (collect at /tmp/ipykernel_14601/2935220721.py:55) finished in 0.060 s\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/28 22:12:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Job 5 finished: collect at /tmp/ipykernel_14601/2935220721.py:55, took 0.061094 s\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Registering RDD 20 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Got map stage job 6 (showString at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/28 22:12:48 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 43.1 KiB, free 434.3 MiB)\n",
      "24/06/28 22:12:48 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 434.3 MiB)\n",
      "24/06/28 22:12:48 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:44785 (size: 19.9 KiB, free: 434.4 MiB)\n",
      "24/06/28 22:12:48 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585\n",
      "24/06/28 22:12:48 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "24/06/28 22:12:48 INFO TaskSchedulerImpl: Adding task set 9.0 with 8 tasks resource profile 0\n",
      "24/06/28 22:12:48 WARN TaskSetManager: Stage 9 contains a task of very large size (1841 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/28 22:12:48 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 20) (127.0.0.1, executor 1, partition 0, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:48 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 21) (127.0.0.1, executor 0, partition 1, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:48 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 22) (127.0.0.1, executor 1, partition 2, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:48 INFO TaskSetManager: Starting task 3.0 in stage 9.0 (TID 23) (127.0.0.1, executor 0, partition 3, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:48 INFO TaskSetManager: Starting task 4.0 in stage 9.0 (TID 24) (127.0.0.1, executor 1, partition 4, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:48 INFO TaskSetManager: Starting task 5.0 in stage 9.0 (TID 25) (127.0.0.1, executor 0, partition 5, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:48 INFO TaskSetManager: Starting task 6.0 in stage 9.0 (TID 26) (127.0.0.1, executor 1, partition 6, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:48 INFO TaskSetManager: Starting task 7.0 in stage 9.0 (TID 27) (127.0.0.1, executor 0, partition 7, PROCESS_LOCAL, 1894328 bytes) \n",
      "24/06/28 22:12:48 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:44785 in memory (size: 8.0 KiB, free: 434.4 MiB)\n",
      "24/06/28 22:12:48 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 127.0.0.1:36899 in memory (size: 8.0 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:48 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 127.0.0.1:42841 (size: 19.9 KiB, free: 2.2 GiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped Statistics by Category:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/28 22:12:48 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:44785 in memory (size: 7.9 KiB, free: 434.4 MiB)\n",
      "24/06/28 22:12:48 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 127.0.0.1:36899 (size: 19.9 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:48 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 127.0.0.1:42841 in memory (size: 7.9 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:48 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:44785 in memory (size: 10.3 KiB, free: 434.4 MiB)\n",
      "24/06/28 22:12:48 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 127.0.0.1:42841 in memory (size: 10.3 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:48 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 127.0.0.1:36899 in memory (size: 10.3 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:49 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 22) in 661 ms on 127.0.0.1 (executor 1) (1/8)\n",
      "24/06/28 22:12:49 INFO TaskSetManager: Finished task 4.0 in stage 9.0 (TID 24) in 660 ms on 127.0.0.1 (executor 1) (2/8)\n",
      "24/06/28 22:12:49 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 20) in 664 ms on 127.0.0.1 (executor 1) (3/8)\n",
      "24/06/28 22:12:49 INFO TaskSetManager: Finished task 6.0 in stage 9.0 (TID 26) in 660 ms on 127.0.0.1 (executor 1) (4/8)\n",
      "24/06/28 22:12:49 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 21) in 703 ms on 127.0.0.1 (executor 0) (5/8)\n",
      "24/06/28 22:12:49 INFO TaskSetManager: Finished task 7.0 in stage 9.0 (TID 27) in 699 ms on 127.0.0.1 (executor 0) (6/8)\n",
      "24/06/28 22:12:49 INFO TaskSetManager: Finished task 5.0 in stage 9.0 (TID 25) in 702 ms on 127.0.0.1 (executor 0) (7/8)\n",
      "24/06/28 22:12:49 INFO TaskSetManager: Finished task 3.0 in stage 9.0 (TID 23) in 704 ms on 127.0.0.1 (executor 0) (8/8)\n",
      "24/06/28 22:12:49 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "24/06/28 22:12:49 INFO DAGScheduler: ShuffleMapStage 9 (showString at NativeMethodAccessorImpl.java:0) finished in 0.711 s\n",
      "24/06/28 22:12:49 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/06/28 22:12:49 INFO DAGScheduler: running: Set()\n",
      "24/06/28 22:12:49 INFO DAGScheduler: waiting: Set()\n",
      "24/06/28 22:12:49 INFO DAGScheduler: failed: Set()\n",
      "24/06/28 22:12:49 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/06/28 22:12:49 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/06/28 22:12:49 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/06/28 22:12:49 INFO DAGScheduler: Got job 7 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/28 22:12:49 INFO DAGScheduler: Final stage: ResultStage 11 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/06/28 22:12:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\n",
      "24/06/28 22:12:49 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/28 22:12:49 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[23] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/28 22:12:49 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 46.9 KiB, free 434.3 MiB)\n",
      "24/06/28 22:12:49 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 21.7 KiB, free 434.3 MiB)\n",
      "24/06/28 22:12:49 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:44785 (size: 21.7 KiB, free: 434.4 MiB)\n",
      "24/06/28 22:12:49 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
      "24/06/28 22:12:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/28 22:12:49 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
      "24/06/28 22:12:49 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 28) (127.0.0.1, executor 0, partition 0, NODE_LOCAL, 7619 bytes) \n",
      "24/06/28 22:12:49 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:36899 (size: 21.7 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:49 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 127.0.0.1:41994\n",
      "24/06/28 22:12:49 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 28) in 80 ms on 127.0.0.1 (executor 0) (1/1)\n",
      "24/06/28 22:12:49 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "24/06/28 22:12:49 INFO DAGScheduler: ResultStage 11 (showString at NativeMethodAccessorImpl.java:0) finished in 0.085 s\n",
      "24/06/28 22:12:49 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/28 22:12:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
      "24/06/28 22:12:49 INFO DAGScheduler: Job 7 finished: showString at NativeMethodAccessorImpl.java:0, took 0.087478 s\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------------+\n",
      "|Category| Count|  Average_Integer|\n",
      "+--------+------+-----------------+\n",
      "|       B|333196| 50.4520972640728|\n",
      "|       C|332430|50.54151851517613|\n",
      "|       A|334374|50.48330611829867|\n",
      "+--------+------+-----------------+\n",
      "\n",
      "Transformed DataFrame with Running Total:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/28 22:12:49 INFO CodeGenerator: Code generated in 7.039687 ms\n",
      "24/06/28 22:12:49 INFO DAGScheduler: Registering RDD 25 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 3\n",
      "24/06/28 22:12:49 INFO DAGScheduler: Got map stage job 8 (showString at NativeMethodAccessorImpl.java:0) with 8 output partitions\n",
      "24/06/28 22:12:49 INFO DAGScheduler: Final stage: ShuffleMapStage 12 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/06/28 22:12:49 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/28 22:12:49 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/28 22:12:49 INFO DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[25] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/28 22:12:49 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 17.6 KiB, free 434.3 MiB)\n",
      "24/06/28 22:12:49 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 434.2 MiB)\n",
      "24/06/28 22:12:49 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:44785 (size: 9.0 KiB, free: 434.4 MiB)\n",
      "24/06/28 22:12:49 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585\n",
      "24/06/28 22:12:49 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[25] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))\n",
      "24/06/28 22:12:49 INFO TaskSchedulerImpl: Adding task set 12.0 with 8 tasks resource profile 0\n",
      "24/06/28 22:12:49 WARN TaskSetManager: Stage 12 contains a task of very large size (1841 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/06/28 22:12:49 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 29) (127.0.0.1, executor 0, partition 0, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:49 INFO TaskSetManager: Starting task 1.0 in stage 12.0 (TID 30) (127.0.0.1, executor 1, partition 1, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:49 INFO TaskSetManager: Starting task 2.0 in stage 12.0 (TID 31) (127.0.0.1, executor 0, partition 2, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:49 INFO TaskSetManager: Starting task 3.0 in stage 12.0 (TID 32) (127.0.0.1, executor 1, partition 3, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:49 INFO TaskSetManager: Starting task 4.0 in stage 12.0 (TID 33) (127.0.0.1, executor 0, partition 4, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:49 INFO TaskSetManager: Starting task 5.0 in stage 12.0 (TID 34) (127.0.0.1, executor 1, partition 5, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:49 INFO TaskSetManager: Starting task 6.0 in stage 12.0 (TID 35) (127.0.0.1, executor 0, partition 6, PROCESS_LOCAL, 1885656 bytes) \n",
      "24/06/28 22:12:49 INFO TaskSetManager: Starting task 7.0 in stage 12.0 (TID 36) (127.0.0.1, executor 1, partition 7, PROCESS_LOCAL, 1894328 bytes) \n",
      "24/06/28 22:12:49 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 127.0.0.1:36899 (size: 9.0 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:49 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 127.0.0.1:42841 (size: 9.0 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:49 INFO TaskSetManager: Finished task 6.0 in stage 12.0 (TID 35) in 402 ms on 127.0.0.1 (executor 0) (1/8)\n",
      "24/06/28 22:12:49 INFO TaskSetManager: Finished task 2.0 in stage 12.0 (TID 31) in 424 ms on 127.0.0.1 (executor 0) (2/8)\n",
      "24/06/28 22:12:49 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 29) in 429 ms on 127.0.0.1 (executor 0) (3/8)\n",
      "24/06/28 22:12:49 INFO TaskSetManager: Finished task 4.0 in stage 12.0 (TID 33) in 441 ms on 127.0.0.1 (executor 0) (4/8)\n",
      "24/06/28 22:12:49 INFO TaskSetManager: Finished task 5.0 in stage 12.0 (TID 34) in 462 ms on 127.0.0.1 (executor 1) (5/8)\n",
      "24/06/28 22:12:49 INFO TaskSetManager: Finished task 7.0 in stage 12.0 (TID 36) in 467 ms on 127.0.0.1 (executor 1) (6/8)\n",
      "24/06/28 22:12:49 INFO TaskSetManager: Finished task 3.0 in stage 12.0 (TID 32) in 477 ms on 127.0.0.1 (executor 1) (7/8)\n",
      "24/06/28 22:12:49 INFO TaskSetManager: Finished task 1.0 in stage 12.0 (TID 30) in 487 ms on 127.0.0.1 (executor 1) (8/8)\n",
      "24/06/28 22:12:49 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "24/06/28 22:12:49 INFO DAGScheduler: ShuffleMapStage 12 (showString at NativeMethodAccessorImpl.java:0) finished in 0.494 s\n",
      "24/06/28 22:12:49 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/06/28 22:12:49 INFO DAGScheduler: running: Set()\n",
      "24/06/28 22:12:49 INFO DAGScheduler: waiting: Set()\n",
      "24/06/28 22:12:49 INFO DAGScheduler: failed: Set()\n",
      "24/06/28 22:12:49 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 4027580, minimum partition size: 1048576\n",
      "24/06/28 22:12:49 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/06/28 22:12:49 INFO DAGScheduler: Got job 9 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/06/28 22:12:49 INFO DAGScheduler: Final stage: ResultStage 14 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/06/28 22:12:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\n",
      "24/06/28 22:12:49 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/28 22:12:49 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[30] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/06/28 22:12:49 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 41.7 KiB, free 434.2 MiB)\n",
      "24/06/28 22:12:49 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 19.5 KiB, free 434.2 MiB)\n",
      "24/06/28 22:12:49 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:44785 (size: 19.5 KiB, free: 434.3 MiB)\n",
      "24/06/28 22:12:49 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:44785 in memory (size: 19.9 KiB, free: 434.4 MiB)\n",
      "24/06/28 22:12:49 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585\n",
      "24/06/28 22:12:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[30] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/28 22:12:49 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
      "24/06/28 22:12:49 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 37) (127.0.0.1, executor 1, partition 0, NODE_LOCAL, 7619 bytes) \n",
      "24/06/28 22:12:49 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 127.0.0.1:42841 in memory (size: 19.9 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:49 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 127.0.0.1:36899 in memory (size: 19.9 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:49 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:44785 in memory (size: 9.0 KiB, free: 434.4 MiB)\n",
      "24/06/28 22:12:49 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 127.0.0.1:42841 in memory (size: 9.0 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:49 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 127.0.0.1:36899 in memory (size: 9.0 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:49 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 127.0.0.1:42841 (size: 19.5 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:49 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:44785 in memory (size: 21.7 KiB, free: 434.4 MiB)\n",
      "24/06/28 22:12:49 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 127.0.0.1:36899 in memory (size: 21.7 KiB, free: 2.2 GiB)\n",
      "24/06/28 22:12:49 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 127.0.0.1:42000\n",
      "24/06/28 22:12:50 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 37) in 1028 ms on 127.0.0.1 (executor 1) (1/1)\n",
      "24/06/28 22:12:50 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "24/06/28 22:12:50 INFO DAGScheduler: ResultStage 14 (showString at NativeMethodAccessorImpl.java:0) finished in 1.039 s\n",
      "24/06/28 22:12:50 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/28 22:12:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
      "24/06/28 22:12:50 INFO DAGScheduler: Job 9 finished: showString at NativeMethodAccessorImpl.java:0, took 1.040871 s\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------+--------------------+------------------+------------+\n",
      "|IntegerColumn|        FloatColumn|Category|     NormalizedFloat| ComplexExpression|RunningTotal|\n",
      "+-------------+-------------------+--------+--------------------+------------------+------------+\n",
      "|            1| 0.4138824354331524|       B|  -0.299983544068513|1.3177333169553478|           1|\n",
      "|            1|0.04896025268316473|       B| -1.5640066903121579| 1.047742380769839|           2|\n",
      "|            1| 0.4577844569536277|       B|-0.14791503743353437| 1.338995659788864|           3|\n",
      "|            1|0.23914566324267317|       B|  -0.905239499966349| 1.208413388476698|           4|\n",
      "|            1| 0.7820474784777913|       B|  0.9752722722243572| 1.414205623629453|           5|\n",
      "+-------------+-------------------+--------+--------------------+------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, stddev, expr, count, sum as Fsum\n",
    "from pyspark.sql.window import Window\n",
    "import random\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Set up your Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Advanced PySpark Test\") \\\n",
    "    .master(\"spark://localhost:7077\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .config(\"spark.driver.port\", \"7078\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.local.dir\", \"/tmp/spark-temp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to INFO\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "# Print the Spark version to verify the connection\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 1000000\n",
    "data = [(random.randint(1, 100), random.random(), random.choice(['A', 'B', 'C'])) for _ in range(num_samples)]\n",
    "\n",
    "# Define schema\n",
    "columns = [\"IntegerColumn\", \"FloatColumn\", \"Category\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "print(\"Initial DataFrame:\")\n",
    "df.show(5)\n",
    "\n",
    "# Perform transformations\n",
    "# 1. Compute average and standard deviation of FloatColumn\n",
    "stats = df.agg(\n",
    "    avg(\"FloatColumn\").alias(\"Average_Float\"),\n",
    "    stddev(\"FloatColumn\").alias(\"StdDev_Float\")\n",
    ")\n",
    "\n",
    "print(\"Statistics of FloatColumn:\")\n",
    "stats.show()\n",
    "\n",
    "# 2. Add a new column with normalized values of FloatColumn\n",
    "average_float = stats.collect()[0][\"Average_Float\"]\n",
    "stddev_float = stats.collect()[0][\"StdDev_Float\"]\n",
    "\n",
    "df = df.withColumn(\"NormalizedFloat\", (col(\"FloatColumn\") - average_float) / stddev_float)\n",
    "\n",
    "# 3. Group by Category and compute count and average of IntegerColumn\n",
    "grouped_stats = df.groupBy(\"Category\").agg(\n",
    "    count(\"IntegerColumn\").alias(\"Count\"),\n",
    "    avg(\"IntegerColumn\").alias(\"Average_Integer\")\n",
    ")\n",
    "\n",
    "print(\"Grouped Statistics by Category:\")\n",
    "grouped_stats.show()\n",
    "\n",
    "# 4. Apply a complex expression to create a new column\n",
    "df = df.withColumn(\"ComplexExpression\", expr(\"IntegerColumn * sin(FloatColumn) + cos(FloatColumn)\"))\n",
    "\n",
    "# 5. Use window functions to compute running total of IntegerColumn partitioned by Category\n",
    "window_spec = Window.partitionBy(\"Category\").orderBy(\"IntegerColumn\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df = df.withColumn(\"RunningTotal\", Fsum(\"IntegerColumn\").over(window_spec))\n",
    "\n",
    "print(\"Transformed DataFrame with Running Total:\")\n",
    "df.show(5)\n",
    "\n",
    "# 6. Machine Learning - Linear Regression\n",
    "# Generate synthetic data for regression\n",
    "ml_data = [(random.random(), random.random() * 10 + random.random()) for _ in range(num_samples)]\n",
    "ml_columns = [\"Feature\", \"Label\"]\n",
    "\n",
    "ml_df = spark.createDataFrame(ml_data, ml_columns)\n",
    "\n",
    "# Assemble features\n",
    "vector_assembler = VectorAssembler(inputCols=[\"Feature\"], outputCol=\"features\")\n",
    "ml_df = vector_assembler.transform(ml_df)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_data, test_data = ml_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train a linear regression model\n",
    "lr = LinearRegression(featuresCol='features', labelCol='Label')\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Print model coefficients and intercept\n",
    "print(\"Linear Regression Coefficients: \", lr_model.coefficients)\n",
    "print(\"Linear Regression Intercept: \", lr_model.intercept)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "predictions.select(\"Feature\", \"Label\", \"prediction\").show(5)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Label\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# 7. Machine Learning - Clustering with K-Means\n",
    "# Generate synthetic data for clustering\n",
    "clustering_data = [(random.random(), random.random()) for _ in range(num_samples)]\n",
    "clustering_columns = [\"Feature1\", \"Feature2\"]\n",
    "\n",
    "clustering_df = spark.createDataFrame(clustering_data, clustering_columns)\n",
    "\n",
    "# Assemble features for clustering\n",
    "vector_assembler_clustering = VectorAssembler(inputCols=[\"Feature1\", \"Feature2\"], outputCol=\"features\")\n",
    "clustering_df = vector_assembler_clustering.transform(clustering_df)\n",
    "\n",
    "# Train a K-Means model\n",
    "kmeans = KMeans(k=3, seed=1)\n",
    "kmeans_model = kmeans.fit(clustering_df)\n",
    "\n",
    "# Make predictions\n",
    "cluster_predictions = kmeans_model.transform(clustering_df)\n",
    "cluster_predictions.select(\"Feature1\", \"Feature2\", \"prediction\").show(5)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b05e2d-4d86-4650-962e-51591c3775b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
